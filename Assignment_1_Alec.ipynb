{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2020 Semester 1\n",
    "\n",
    "## Assignment 1: Naive Bayes Classifiers\n",
    "\n",
    "###### Submission deadline: 7 pm, Monday 20 Apr 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Name(s):**    `Alec Yu, Michael Jaworski`\n",
    "\n",
    "**Student ID(s):**     `993433, `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for all the datafiles and their types\n",
    "nominal_files = [\"breast-cancer-wisconsin\", \"mushroom\", \"lymphography\", \"train\"]\n",
    "numeric_files = [\"wdbc\", \"wine\"]\n",
    "ordinal_files = [\"car\", \"nursery\", \"somerville\"]\n",
    "mixed_files = [\"adult\", \"bank\", \"university\"]\n",
    "\n",
    "# use trainingtest\" for naive bayes example in slides\n",
    "file = \"wine\"\n",
    "filename = \"datasets/%s.data\" % file\n",
    "headerfile = \"datasets/%s.h\" % file\n",
    "\n",
    "if(file in nominal_files):\n",
    "    datatype = \"nominal\"\n",
    "elif (file in ordinal_files):\n",
    "    datatype = \"ordinal\"\n",
    "elif (file in numeric_files):\n",
    "    datatype = \"numeric\"\n",
    "elif (file in mixed_files):\n",
    "    datatype = \"mixed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the datafile in with a separate header file for attributes\n",
    "dataframe = pd.read_csv(filename, header=None)\n",
    "header = open(headerfile, \"r\")\n",
    "attributes = header.readline().split(\",\")\n",
    "dataframe.columns = attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alecy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.640000</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.380000</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.680000</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.320000</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.39</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.6</td>\n",
       "      <td>96</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>1.02</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1290</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.61</td>\n",
       "      <td>17.6</td>\n",
       "      <td>121</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.050000</td>\n",
       "      <td>1.06</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1295</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.27</td>\n",
       "      <td>16.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.85</td>\n",
       "      <td>7.220000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1045</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14.10</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.30</td>\n",
       "      <td>18.0</td>\n",
       "      <td>105</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.12</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.32</td>\n",
       "      <td>16.8</td>\n",
       "      <td>95</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.57</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.41</td>\n",
       "      <td>16.0</td>\n",
       "      <td>89</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.81</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1320</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.39</td>\n",
       "      <td>11.4</td>\n",
       "      <td>91</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1150</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.38</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>102</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.96</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1547</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13.63</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.70</td>\n",
       "      <td>17.2</td>\n",
       "      <td>112</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.46</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.30</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.72</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.07</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13.83</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.62</td>\n",
       "      <td>20.0</td>\n",
       "      <td>115</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.72</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.19</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.48</td>\n",
       "      <td>16.5</td>\n",
       "      <td>108</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.86</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.64</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2.56</td>\n",
       "      <td>15.2</td>\n",
       "      <td>116</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.66</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.36</td>\n",
       "      <td>845</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.06</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>126</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.10</td>\n",
       "      <td>5.650000</td>\n",
       "      <td>1.09</td>\n",
       "      <td>3.71</td>\n",
       "      <td>780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12.93</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.65</td>\n",
       "      <td>18.6</td>\n",
       "      <td>102</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.52</td>\n",
       "      <td>770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13.71</td>\n",
       "      <td>1.86</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.88</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.69</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.11</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1035</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12.85</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.52</td>\n",
       "      <td>17.8</td>\n",
       "      <td>95</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.46</td>\n",
       "      <td>3.930000</td>\n",
       "      <td>1.09</td>\n",
       "      <td>3.63</td>\n",
       "      <td>1015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13.50</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.61</td>\n",
       "      <td>20.0</td>\n",
       "      <td>96</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.82</td>\n",
       "      <td>845</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13.05</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.22</td>\n",
       "      <td>25.0</td>\n",
       "      <td>124</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>1.13</td>\n",
       "      <td>3.20</td>\n",
       "      <td>830</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.39</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.62</td>\n",
       "      <td>16.1</td>\n",
       "      <td>93</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.45</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.92</td>\n",
       "      <td>3.22</td>\n",
       "      <td>1195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.30</td>\n",
       "      <td>1.72</td>\n",
       "      <td>2.14</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>1.02</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1285</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.87</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.80</td>\n",
       "      <td>19.4</td>\n",
       "      <td>107</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.76</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.40</td>\n",
       "      <td>915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>14.02</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.21</td>\n",
       "      <td>16.0</td>\n",
       "      <td>96</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.98</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1035</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>13.32</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2.38</td>\n",
       "      <td>21.5</td>\n",
       "      <td>92</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.25</td>\n",
       "      <td>8.420000</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.62</td>\n",
       "      <td>650</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>13.08</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.36</td>\n",
       "      <td>21.5</td>\n",
       "      <td>113</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.33</td>\n",
       "      <td>550</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>13.50</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.62</td>\n",
       "      <td>24.0</td>\n",
       "      <td>123</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.25</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.30</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>12.79</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.48</td>\n",
       "      <td>22.0</td>\n",
       "      <td>112</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.26</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.47</td>\n",
       "      <td>480</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>13.11</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.75</td>\n",
       "      <td>25.5</td>\n",
       "      <td>116</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.56</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.33</td>\n",
       "      <td>425</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>13.23</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.28</td>\n",
       "      <td>18.5</td>\n",
       "      <td>98</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.87</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.51</td>\n",
       "      <td>675</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>12.58</td>\n",
       "      <td>1.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>103</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.40</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.55</td>\n",
       "      <td>640</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>13.17</td>\n",
       "      <td>5.19</td>\n",
       "      <td>2.32</td>\n",
       "      <td>22.0</td>\n",
       "      <td>93</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.55</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.48</td>\n",
       "      <td>725</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>13.84</td>\n",
       "      <td>4.12</td>\n",
       "      <td>2.38</td>\n",
       "      <td>19.5</td>\n",
       "      <td>89</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.56</td>\n",
       "      <td>9.010000</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.64</td>\n",
       "      <td>480</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>12.45</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.64</td>\n",
       "      <td>27.0</td>\n",
       "      <td>97</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.14</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.73</td>\n",
       "      <td>880</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>14.34</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.70</td>\n",
       "      <td>25.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2.70</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.96</td>\n",
       "      <td>660</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>13.48</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.64</td>\n",
       "      <td>22.5</td>\n",
       "      <td>89</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.29</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.78</td>\n",
       "      <td>620</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>12.36</td>\n",
       "      <td>3.83</td>\n",
       "      <td>2.38</td>\n",
       "      <td>21.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.04</td>\n",
       "      <td>7.650000</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.58</td>\n",
       "      <td>520</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>13.69</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.54</td>\n",
       "      <td>20.0</td>\n",
       "      <td>107</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.80</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.82</td>\n",
       "      <td>680</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>12.85</td>\n",
       "      <td>3.27</td>\n",
       "      <td>2.58</td>\n",
       "      <td>22.0</td>\n",
       "      <td>106</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.96</td>\n",
       "      <td>5.580000</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2.11</td>\n",
       "      <td>570</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>12.96</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.35</td>\n",
       "      <td>18.5</td>\n",
       "      <td>106</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.94</td>\n",
       "      <td>5.280000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.75</td>\n",
       "      <td>675</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>13.78</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.30</td>\n",
       "      <td>22.0</td>\n",
       "      <td>90</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.03</td>\n",
       "      <td>9.580000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.68</td>\n",
       "      <td>615</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>13.73</td>\n",
       "      <td>4.36</td>\n",
       "      <td>2.26</td>\n",
       "      <td>22.5</td>\n",
       "      <td>88</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.15</td>\n",
       "      <td>6.620000</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.75</td>\n",
       "      <td>520</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>13.45</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.60</td>\n",
       "      <td>23.0</td>\n",
       "      <td>111</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.46</td>\n",
       "      <td>10.680000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.56</td>\n",
       "      <td>695</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>12.82</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.30</td>\n",
       "      <td>19.5</td>\n",
       "      <td>88</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.97</td>\n",
       "      <td>10.260000</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.75</td>\n",
       "      <td>685</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>13.58</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.69</td>\n",
       "      <td>24.5</td>\n",
       "      <td>105</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.54</td>\n",
       "      <td>8.660000</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.80</td>\n",
       "      <td>750</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>13.40</td>\n",
       "      <td>4.60</td>\n",
       "      <td>2.86</td>\n",
       "      <td>25.0</td>\n",
       "      <td>112</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.11</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.92</td>\n",
       "      <td>630</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>12.20</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.32</td>\n",
       "      <td>19.0</td>\n",
       "      <td>96</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.73</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.83</td>\n",
       "      <td>510</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>12.77</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.28</td>\n",
       "      <td>19.5</td>\n",
       "      <td>86</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.64</td>\n",
       "      <td>9.899999</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.63</td>\n",
       "      <td>470</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>14.16</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.48</td>\n",
       "      <td>20.0</td>\n",
       "      <td>91</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.24</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.71</td>\n",
       "      <td>660</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat1  feat2  feat3  feat4  feat5  feat6  feat7  feat8  feat9     feat10  \\\n",
       "0    14.23   1.71   2.43   15.6    127   2.80   3.06   0.28   2.29   5.640000   \n",
       "1    13.20   1.78   2.14   11.2    100   2.65   2.76   0.26   1.28   4.380000   \n",
       "2    13.16   2.36   2.67   18.6    101   2.80   3.24   0.30   2.81   5.680000   \n",
       "3    14.37   1.95   2.50   16.8    113   3.85   3.49   0.24   2.18   7.800000   \n",
       "4    13.24   2.59   2.87   21.0    118   2.80   2.69   0.39   1.82   4.320000   \n",
       "5    14.20   1.76   2.45   15.2    112   3.27   3.39   0.34   1.97   6.750000   \n",
       "6    14.39   1.87   2.45   14.6     96   2.50   2.52   0.30   1.98   5.250000   \n",
       "7    14.06   2.15   2.61   17.6    121   2.60   2.51   0.31   1.25   5.050000   \n",
       "8    14.83   1.64   2.17   14.0     97   2.80   2.98   0.29   1.98   5.200000   \n",
       "9    13.86   1.35   2.27   16.0     98   2.98   3.15   0.22   1.85   7.220000   \n",
       "10   14.10   2.16   2.30   18.0    105   2.95   3.32   0.22   2.38   5.750000   \n",
       "11   14.12   1.48   2.32   16.8     95   2.20   2.43   0.26   1.57   5.000000   \n",
       "12   13.75   1.73   2.41   16.0     89   2.60   2.76   0.29   1.81   5.600000   \n",
       "13   14.75   1.73   2.39   11.4     91   3.10   3.69   0.43   2.81   5.400000   \n",
       "14   14.38   1.87   2.38   12.0    102   3.30   3.64   0.29   2.96   7.500000   \n",
       "15   13.63   1.81   2.70   17.2    112   2.85   2.91   0.30   1.46   7.300000   \n",
       "16   14.30   1.92   2.72   20.0    120   2.80   3.14   0.33   1.97   6.200000   \n",
       "17   13.83   1.57   2.62   20.0    115   2.95   3.40   0.40   1.72   6.600000   \n",
       "18   14.19   1.59   2.48   16.5    108   3.30   3.93   0.32   1.86   8.700000   \n",
       "19   13.64   3.10   2.56   15.2    116   2.70   3.03   0.17   1.66   5.100000   \n",
       "20   14.06   1.63   2.28   16.0    126   3.00   3.17   0.24   2.10   5.650000   \n",
       "21   12.93   3.80   2.65   18.6    102   2.41   2.41   0.25   1.98   4.500000   \n",
       "22   13.71   1.86   2.36   16.6    101   2.61   2.88   0.27   1.69   3.800000   \n",
       "23   12.85   1.60   2.52   17.8     95   2.48   2.37   0.26   1.46   3.930000   \n",
       "24   13.50   1.81   2.61   20.0     96   2.53   2.61   0.28   1.66   3.520000   \n",
       "25   13.05   2.05   3.22   25.0    124   2.63   2.68   0.47   1.92   3.580000   \n",
       "26   13.39   1.77   2.62   16.1     93   2.85   2.94   0.34   1.45   4.800000   \n",
       "27   13.30   1.72   2.14   17.0     94   2.40   2.19   0.27   1.35   3.950000   \n",
       "28   13.87   1.90   2.80   19.4    107   2.95   2.97   0.37   1.76   4.500000   \n",
       "29   14.02   1.68   2.21   16.0     96   2.65   2.33   0.26   1.98   4.700000   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...        ...   \n",
       "148  13.32   3.24   2.38   21.5     92   1.93   0.76   0.45   1.25   8.420000   \n",
       "149  13.08   3.90   2.36   21.5    113   1.41   1.39   0.34   1.14   9.400000   \n",
       "150  13.50   3.12   2.62   24.0    123   1.40   1.57   0.22   1.25   8.600000   \n",
       "151  12.79   2.67   2.48   22.0    112   1.48   1.36   0.24   1.26  10.800000   \n",
       "152  13.11   1.90   2.75   25.5    116   2.20   1.28   0.26   1.56   7.100000   \n",
       "153  13.23   3.30   2.28   18.5     98   1.80   0.83   0.61   1.87  10.520000   \n",
       "154  12.58   1.29   2.10   20.0    103   1.48   0.58   0.53   1.40   7.600000   \n",
       "155  13.17   5.19   2.32   22.0     93   1.74   0.63   0.61   1.55   7.900000   \n",
       "156  13.84   4.12   2.38   19.5     89   1.80   0.83   0.48   1.56   9.010000   \n",
       "157  12.45   3.03   2.64   27.0     97   1.90   0.58   0.63   1.14   7.500000   \n",
       "158  14.34   1.68   2.70   25.0     98   2.80   1.31   0.53   2.70  13.000000   \n",
       "159  13.48   1.67   2.64   22.5     89   2.60   1.10   0.52   2.29  11.750000   \n",
       "160  12.36   3.83   2.38   21.0     88   2.30   0.92   0.50   1.04   7.650000   \n",
       "161  13.69   3.26   2.54   20.0    107   1.83   0.56   0.50   0.80   5.880000   \n",
       "162  12.85   3.27   2.58   22.0    106   1.65   0.60   0.60   0.96   5.580000   \n",
       "163  12.96   3.45   2.35   18.5    106   1.39   0.70   0.40   0.94   5.280000   \n",
       "164  13.78   2.76   2.30   22.0     90   1.35   0.68   0.41   1.03   9.580000   \n",
       "165  13.73   4.36   2.26   22.5     88   1.28   0.47   0.52   1.15   6.620000   \n",
       "166  13.45   3.70   2.60   23.0    111   1.70   0.92   0.43   1.46  10.680000   \n",
       "167  12.82   3.37   2.30   19.5     88   1.48   0.66   0.40   0.97  10.260000   \n",
       "168  13.58   2.58   2.69   24.5    105   1.55   0.84   0.39   1.54   8.660000   \n",
       "169  13.40   4.60   2.86   25.0    112   1.98   0.96   0.27   1.11   8.500000   \n",
       "170  12.20   3.03   2.32   19.0     96   1.25   0.49   0.40   0.73   5.500000   \n",
       "171  12.77   2.39   2.28   19.5     86   1.39   0.51   0.48   0.64   9.899999   \n",
       "172  14.16   2.51   2.48   20.0     91   1.68   0.70   0.44   1.24   9.700000   \n",
       "173  13.71   5.65   2.45   20.5     95   1.68   0.61   0.52   1.06   7.700000   \n",
       "174  13.40   3.91   2.48   23.0    102   1.80   0.75   0.43   1.41   7.300000   \n",
       "175  13.27   4.28   2.26   20.0    120   1.59   0.69   0.43   1.35  10.200000   \n",
       "176  13.17   2.59   2.37   20.0    120   1.65   0.68   0.53   1.46   9.300000   \n",
       "177  14.13   4.10   2.74   24.5     96   2.05   0.76   0.56   1.35   9.200000   \n",
       "\n",
       "     feat11  feat12  feat13  CLASS  \n",
       "0      1.04    3.92    1065      1  \n",
       "1      1.05    3.40    1050      1  \n",
       "2      1.03    3.17    1185      1  \n",
       "3      0.86    3.45    1480      1  \n",
       "4      1.04    2.93     735      1  \n",
       "5      1.05    2.85    1450      1  \n",
       "6      1.02    3.58    1290      1  \n",
       "7      1.06    3.58    1295      1  \n",
       "8      1.08    2.85    1045      1  \n",
       "9      1.01    3.55    1045      1  \n",
       "10     1.25    3.17    1510      1  \n",
       "11     1.17    2.82    1280      1  \n",
       "12     1.15    2.90    1320      1  \n",
       "13     1.25    2.73    1150      1  \n",
       "14     1.20    3.00    1547      1  \n",
       "15     1.28    2.88    1310      1  \n",
       "16     1.07    2.65    1280      1  \n",
       "17     1.13    2.57    1130      1  \n",
       "18     1.23    2.82    1680      1  \n",
       "19     0.96    3.36     845      1  \n",
       "20     1.09    3.71     780      1  \n",
       "21     1.03    3.52     770      1  \n",
       "22     1.11    4.00    1035      1  \n",
       "23     1.09    3.63    1015      1  \n",
       "24     1.12    3.82     845      1  \n",
       "25     1.13    3.20     830      1  \n",
       "26     0.92    3.22    1195      1  \n",
       "27     1.02    2.77    1285      1  \n",
       "28     1.25    3.40     915      1  \n",
       "29     1.04    3.59    1035      1  \n",
       "..      ...     ...     ...    ...  \n",
       "148    0.55    1.62     650      3  \n",
       "149    0.57    1.33     550      3  \n",
       "150    0.59    1.30     500      3  \n",
       "151    0.48    1.47     480      3  \n",
       "152    0.61    1.33     425      3  \n",
       "153    0.56    1.51     675      3  \n",
       "154    0.58    1.55     640      3  \n",
       "155    0.60    1.48     725      3  \n",
       "156    0.57    1.64     480      3  \n",
       "157    0.67    1.73     880      3  \n",
       "158    0.57    1.96     660      3  \n",
       "159    0.57    1.78     620      3  \n",
       "160    0.56    1.58     520      3  \n",
       "161    0.96    1.82     680      3  \n",
       "162    0.87    2.11     570      3  \n",
       "163    0.68    1.75     675      3  \n",
       "164    0.70    1.68     615      3  \n",
       "165    0.78    1.75     520      3  \n",
       "166    0.85    1.56     695      3  \n",
       "167    0.72    1.75     685      3  \n",
       "168    0.74    1.80     750      3  \n",
       "169    0.67    1.92     630      3  \n",
       "170    0.66    1.83     510      3  \n",
       "171    0.57    1.63     470      3  \n",
       "172    0.62    1.71     660      3  \n",
       "173    0.64    1.74     740      3  \n",
       "174    0.70    1.56     750      3  \n",
       "175    0.59    1.56     835      3  \n",
       "176    0.60    1.62     840      3  \n",
       "177    0.61    1.60     560      3  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "\n",
    "def preprocess(dataframe, datatype): \n",
    "    # Create a copy of the original dataframe. Remove rows with ? in this copy\n",
    "    df = dataframe\n",
    "\n",
    "    # If it's mushroom data, drop the stalk-root column as it has too many missing values. Don't use this attribute\n",
    "    if(filename == \"datasets/mushroom.data\"):\n",
    "        df = df.drop(columns = ['stalk-root'])\n",
    "\n",
    "    # Otherwise just remove the entries with a ? for now\n",
    "    for index in df.index:\n",
    "        if('?' in df.loc[index].values):\n",
    "            df = df.drop(index)\n",
    "    \n",
    "    # Leave nominal, ordinal and numeric data as it is.\n",
    "    \n",
    "    # If the data is mixed, we need to discretise the data, as each mixed data set is a classification task using NB\n",
    "    if(datatype == 'mixed'):\n",
    "        df = discretiseNumeric(df)\n",
    "        \n",
    "    # Also, move CLASS column to the end\n",
    "    temp = df.pop('CLASS')\n",
    "    df['CLASS'] = temp\n",
    "    \n",
    "    return df\n",
    "\n",
    "############################################################################################################################\n",
    "# Choice is to use equal-frequency discretisation. More detailed than equal-width discretisation. Here, computing an \n",
    "# \"Optimal\" number of clusters for k means clustering for every numeric column would not be feasible. If choosing an \n",
    "# Arbitrary K clusters for every single discretisation, then it isn't as effective\n",
    "# For equal-frequency, sometimes the attributes won't have many values, so migth be less than 5 bins. Thats okay though\n",
    "\n",
    "def discretiseNumeric(df):\n",
    "    for column in df.columns:\n",
    "        if(df.dtypes[column] == \"int64\" or df.dtypes[column] == \"float64\"):\n",
    "            values = df[column].values\n",
    "            values.sort()\n",
    "            \n",
    "            # Check if the values are already \"Discretised\" (<, say, 8 unique values. This occurs for ratings 0-5 etc..).\n",
    "            # If so, then no need to discretise the attribute\n",
    "            uniqueValues = set(values)\n",
    "            if(len(uniqueValues) <= 8):\n",
    "                continue\n",
    "            \n",
    "            # Choose 5 equal-frequency bins, based on sorted values\n",
    "            # Should every attribute be split into 5 bins?\n",
    "            \n",
    "            bin1 = values[(int)(len(values)/5)]\n",
    "            bin2 = values[2 * (int)(len(values)/5)]\n",
    "            bin3 = values[3 * (int)(len(values)/5)]\n",
    "            bin4 = values[4 * (int)(len(values)/5)]\n",
    "            bin5 = values[5 * (int)(len(values)/5)]\n",
    "            \n",
    "            # Now assign every value to be in a bin number\n",
    "            for i in range(len(df[column].values)):\n",
    "                if(df[column].values[i] <= bin1):\n",
    "                    df[column].values[i] = 1\n",
    "                elif(df[column].values[i] <= bin2):\n",
    "                    df[column].values[i] = 2\n",
    "                elif(df[column].values[i] <= bin3):\n",
    "                    df[column].values[i] = 3\n",
    "                elif(df[column].values[i] <= bin4):\n",
    "                    df[column].values[i] = 4\n",
    "                elif(df[column].values[i] <= bin5):\n",
    "                    df[column].values[i] = 5\n",
    "                \n",
    "    return df\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "df2 = dataframe.copy()\n",
    "df2 = preprocess(df2, datatype)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculate prior probabilities and likelihoods from the training data and using\n",
    "# them to build a naive Bayes model\n",
    "\n",
    "def train(dataframe, datatype):\n",
    "    \n",
    "    if(datatype in [\"nominal\", \"ordinal\", \"mixed\"]):\n",
    "        modeltype = 'NB'\n",
    "    elif(datatype in [\"numeric\"]):\n",
    "        modeltype = 'GNB'\n",
    "    \n",
    "    if(modeltype == 'NB'):\n",
    "        trainNB(dataframe)\n",
    "    elif(modeltype == 'GNB'):\n",
    "        trainGNB(dataframe)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(dataframe): \n",
    "    \n",
    "    # Count all the frequencies of each class label\n",
    "    priors = dict()\n",
    "    for index in dataframe.index:\n",
    "        item = dataframe.loc[index]['CLASS']\n",
    "        if(item in priors):\n",
    "            priors[item] += 1\n",
    "        else:\n",
    "            priors[item] = 1\n",
    "            \n",
    "    # Sum the total to convert each class label to a probability\n",
    "    total = sum(priors.values())\n",
    "    \n",
    "    # Each unique class label now has a prior probability as a value\n",
    "    for key, value in priors.items():\n",
    "        priors[key] = value/total\n",
    "        \n",
    "    # Once we have the priors, we need all the conditional probabilities\n",
    "    # Use a list of dictionaries of dictionaries\n",
    "    \n",
    "    # We need these to iterate through them and count instances\n",
    "    attributes = list(dataframe.columns)[:-1]\n",
    "    uniqueClassLabels = list(set(dataframe['CLASS'].values))\n",
    "    \n",
    "    attributeDicts = list()\n",
    "    \n",
    "    # For each attribute create a dictionary with class labels as keys.\n",
    "    # Values are another dictionary with unique values in that attribute,\n",
    "    # Then count frequencies of uniqueattvalues given class label\n",
    "    for attribute in attributes:\n",
    "        dict1 = dict()\n",
    "        uniqueAttributeValues = list(set(dataframe[attribute].values))\n",
    "        for classLabel in uniqueClassLabels:\n",
    "            dict2 = dict()\n",
    "            dict1[classLabel] = dict2\n",
    "            for attributeValue in uniqueAttributeValues:\n",
    "                dict2[attributeValue] = dataframe[(dataframe[attribute] == attributeValue) & (dataframe['CLASS'] == classLabel)].shape[0]\n",
    "        attributeDicts.append(dict1)\n",
    "    \n",
    "    # Once we have the frequencies, can just convert each to be probabilities\n",
    "    for attributedict in attributeDicts:\n",
    "        for classLabel, classLabelDict in attributedict.items():\n",
    "            totalFreq = sum(classLabelDict.values())\n",
    "            for attributeLevel, freq in classLabelDict.items():\n",
    "                classLabelDict[attributeLevel] = freq / totalFreq\n",
    "\n",
    "    # EACH DICTIONARY IN attributeDicts IS IN ORDER OF THE HEADER COLUMNS\n",
    "    i = 0;\n",
    "    for dictionary in attributeDicts:\n",
    "        print(attributes[i])\n",
    "        i += 1\n",
    "        for classLabel, dict2 in dictionary.items():\n",
    "            print(classLabel)\n",
    "            print(dict2)\n",
    "            print(sum(dict2.values()))\n",
    "    \n",
    "    return attributeDicts, priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGNB(dataframe):\n",
    "        \n",
    "    # Count all the frequencies of each class label\n",
    "    priors = dict()\n",
    "    for index in dataframe.index:\n",
    "        item = dataframe.loc[index]['CLASS']\n",
    "        if(item in priors):\n",
    "            priors[item] += 1\n",
    "        else:\n",
    "            priors[item] = 1\n",
    "            \n",
    "    # Sum the total to convert each class label to a probability\n",
    "    total = sum(priors.values())\n",
    "    \n",
    "    # Each unique class label now has a prior probability as a value\n",
    "    for key, value in priors.items():\n",
    "        priors[key] = value/total\n",
    "        \n",
    "    # Just need to store mean, stdev for every attribute for each unique class label    \n",
    "    meanDict = df2.groupby('CLASS').mean().to_dict()\n",
    "    stdDict = df2.groupby('CLASS').std().to_dict()\n",
    "    \n",
    "    print(meanDict['feat11'])\n",
    "    print(stdDict['feat11'])\n",
    "\n",
    "    return meanDict, stdDict, priors\n",
    "\n",
    "# Calculates probability density given a gaussian dist with mean and std \n",
    "def calculateDensity(mean, std, x):\n",
    "    exponent = math.exp((-1.0/2) * (((x - mean) / std) ** 2))\n",
    "    return (1 / (math.sqrt(2 * math.pi) * std)) * exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1.0620338983050848, 2: 1.0562816901408452, 3: 0.6827083333333334}\n",
      "{1: 0.11648264131468114, 2: 0.20293680811654433, 3: 0.1144410948234819}\n"
     ]
    }
   ],
   "source": [
    "train(df2, datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict classes for new items in a test dataset (for the purposes of this assignment, you\n",
    "# can re-use the training data as a test set\n",
    "\n",
    "# Multiplying probs, need to take logs of probabilities\n",
    "# Smoothe training probabilities\n",
    "# Change format of the probabilities, and ways of accessing them\n",
    "def predict():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the predictions is: 0.20033\n",
      "\n",
      "Confusion Matrix of Truths vs Predicted frequencies\n",
      "\n",
      "Predicted    0    1    2    3    4   All\n",
      "Truths                                  \n",
      "0          120  141  109  119  118   607\n",
      "1          149  111  136  113  113   622\n",
      "2          104  115  116  132  122   589\n",
      "3          113  127  113  119  116   588\n",
      "4          111  119  113  116  135   594\n",
      "All        597  613  587  599  604  3000\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         0         1         2         3         4\n",
      "Truths                                                     \n",
      "0          0.197694  0.226688  0.185059  0.202381  0.198653\n",
      "1          0.245470  0.178457  0.230900  0.192177  0.190236\n",
      "2          0.171334  0.184887  0.196944  0.224490  0.205387\n",
      "3          0.186161  0.204180  0.191851  0.202381  0.195286\n",
      "4          0.182867  0.191318  0.191851  0.197279  0.227273\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This function should evaliate the prediction performance by comparing your modelâ€™s class outputs to ground\n",
    "# truth labels\n",
    "\n",
    "# CODE TO RUN M-CROSS VALIDATION\n",
    "m = 5;\n",
    "width = int(dataframe.shape[0] / m);\n",
    "evaluations = []\n",
    "for i in range(0,m):\n",
    "    testSet = dataframe.index[i*width:(i+1)*width]\n",
    "    trainSet = dataframe.drop(dataframe.index[i*width:(i+1)*width])\n",
    "    # Call predict using thse two dataframes\n",
    "\n",
    "    \n",
    "    \n",
    "# Currently uses Crude total accuracy, confusion matrix of truth vs classified %'s as evaluation metrics\n",
    "# Calculates the prediction accuracy and outputs a confusion matrix of truths vs predictions\n",
    "\n",
    "def evaluate(truthlist, predictions):\n",
    "    # First calculate a crude accuracy score\n",
    "    correct = 0;\n",
    "    wrong = 0;\n",
    "    for i in range(0,len(truthlist)):\n",
    "        if(truthlist[i] == predictions[i]):\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1;\n",
    "    print(\"The accuracy of the predictions is: {:.5f}\\n\".format(correct/(correct + wrong)))\n",
    "        \n",
    "    # Now construct a confusion matrix of each attribute\n",
    "    truthSeries = pd.Series(truthlist, name = \"Truths\")\n",
    "    predictionSeries = pd.Series(predictions, name = \"Predictions\")\n",
    "    confusionDf = pd.crosstab(truthSeries, predictionSeries, rownames=[\"Truths\"], colnames=[\"Predicted\"], margins=True)\n",
    "    print(\"Confusion Matrix of Truths vs Predicted frequencies\\n\")\n",
    "    print(confusionDf)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Now normalise the confusion matrix so its a percentage of classification performance\n",
    "    confusionDf = pd.crosstab(truthSeries, predictionSeries, rownames=[\"Truths\"], colnames=[\"Predicted\"], margins=False)\n",
    "    confusionDfNormalised = confusionDf / confusionDf.sum(axis=1)\n",
    "    print(\"Confusion Matrix of Correctly Labeled Classes %'s\\n\")\n",
    "    print(confusionDfNormalised)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Now print out the color map of the classification performaces\n",
    "    plot_color_map(confusionDfNormalised)\n",
    "    \n",
    "    return\n",
    "\n",
    "# Plots the confusion matrix\n",
    "\n",
    "def plot_color_map(confusionDf):\n",
    "    title = \"Confusion Matrix\"\n",
    "    cmap = plt.cm.gray_r\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.matshow(confusionDf, cmap = cmap, fignum=1)\n",
    "    \n",
    "    # Set the scale (0,1) and fontsize for the color bar\n",
    "    plt.clim(0, 1)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(label=\"Classification Performance\", weight=\"bold\", fontsize=20)\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "    \n",
    "    # Axis information\n",
    "    tick_marks = np.arange(len(confusionDf.columns))\n",
    "    plt.xticks(tick_marks, confusionDf.columns, rotation = 45, fontsize=15)\n",
    "    plt.yticks(tick_marks, confusionDf.index, rotation = -45, fontsize=15)\n",
    "    plt.ylabel(confusionDf.index.name, fontsize=22)\n",
    "    plt.xlabel(confusionDf.columns.name, labelpad = 18, fontsize=22)\n",
    "    \n",
    "    return\n",
    "\n",
    "# Finds the most common class label and returns it. Use this to predict all instances as a baseline\n",
    "def zeroR(df):\n",
    "    classLabel = df['CLASS'].value_counts().idxmax();\n",
    "    return classLabel\n",
    "\n",
    "import random\n",
    "\n",
    "# Test sets. Currently uniformly distributed classes in range (0,5)\n",
    "\n",
    "testTruth = [random.randrange(0,5,1) for i in range(3000)]\n",
    "testPred = [random.randrange(0,5,1) for i in range(3000)]\n",
    "\n",
    "evaluate(testTruth, testPred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "\n",
    "\n",
    "If you are in a group of 1, you will respond to question (1), and **one** other of your choosing (two responses in total).\n",
    "\n",
    "If you are in a group of 2, you will respond to question (1) and question (2), and **two** others of your choosing (four responses in total). \n",
    "\n",
    "A response to a question should take about 100â€“250 words, and make reference to the data wherever possible.\n",
    "\n",
    "#### NOTE: you may develope codes or functions in respond to the question, but your formal answer should be added to a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Try discretising the numeric attributes in these datasets and treating them as discrete variables in the naÂ¨Ä±ve Bayes classifier. You can use a discretisation method of your choice and group the numeric values into any number of levels (but around 3 to 5 levels would probably be a good starting point). Does discretizing the variables improve classification performance, compared to the Gaussian naÂ¨Ä±ve Bayes approach? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Implement a baseline model (e.g., random or 0R) and compare the performance of the naÂ¨Ä±ve Bayes classifier to this baseline on multiple datasets. Discuss why the baseline performance varies across datasets, and to what extent the naÂ¨Ä±ve Bayes classifier improves on the baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Since itâ€™s difficult to model the probabilities of ordinal data, ordinal attributes are often treated as either nominal variables or numeric variables. Compare these strategies on the ordinal datasets provided. Deterimine which approach gives higher classification accuracy and discuss why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a holdâ€“out or crossâ€“validation evaluation strategy (you should implement this yourself and do not simply call existing implementations from `scikit-learn`). How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the naÂ¨Ä±ve Bayes classifier? Explain why, or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "The Gaussian naÂ¨Ä±ve Bayes classifier assumes that numeric attributes come from a Gaussian distribution. Is this assumption always true for the numeric attributes in these datasets? Identify some cases where the Gaussian assumption is violated and describe any evidence (or lack thereof) that this has some effect on the NB classifierâ€™s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
