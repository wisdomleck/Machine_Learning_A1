{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2020 Semester 1\n",
    "\n",
    "## Assignment 1: Naive Bayes Classifiers\n",
    "\n",
    "###### Submission deadline: 7 pm, Monday 20 Apr 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Name(s):**    `Alec Yu, Michael Jaworski`\n",
    "\n",
    "**Student ID(s):**     `993433, `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEMS LIST:\n",
    "- Discretisation into x bins/clusters is always going to be arbitrary? As we are making a general model for any input dataset\n",
    "- Currently just removing all instances with missing values. Maybe implement something to take the mean? (For numeric attributes) Categorical might not make sense to take the mode\n",
    "- Discuss whether to make general preprocessing model or hard coded for each dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for all the datafiles and their types\n",
    "nominal_files = [\"breast-cancer-wisconsin\", \"mushroom\", \"lymphography\"]\n",
    "numeric_files = [\"wdbc\", \"wine\"]\n",
    "ordinal_files = [\"car\", \"nursery\", \"somerville\"]\n",
    "mixed_files = [\"adult\", \"bank\", \"university\"]\n",
    "\n",
    "file = \"trainingtest\"\n",
    "filename = \"datasets/%s.data\" % file\n",
    "headerfile = \"datasets/%s.h\" % file\n",
    "\n",
    "if(file in nominal_files):\n",
    "    datatype = \"nominal\"\n",
    "elif (file in ordinal_files):\n",
    "    datatype = \"ordinal\"\n",
    "elif (file in numeric_files):\n",
    "    datatype = \"numeric\"\n",
    "elif (file in mixed_files):\n",
    "    datatype = \"mixed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the datafile in with a separate header file for attributes\n",
    "dataframe = pd.read_csv(filename, header=None)\n",
    "header = open(headerfile, \"r\")\n",
    "attributes = header.readline().split(\",\")\n",
    "dataframe.columns = attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headache</th>\n",
       "      <th>sore</th>\n",
       "      <th>temperature</th>\n",
       "      <th>cough</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>severe</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>yes</td>\n",
       "      <td>flu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>severe</td>\n",
       "      <td>normal</td>\n",
       "      <td>yes</td>\n",
       "      <td>cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mild</td>\n",
       "      <td>mild</td>\n",
       "      <td>normal</td>\n",
       "      <td>yes</td>\n",
       "      <td>flu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mild</td>\n",
       "      <td>no</td>\n",
       "      <td>normal</td>\n",
       "      <td>no</td>\n",
       "      <td>cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>severe</td>\n",
       "      <td>severe</td>\n",
       "      <td>normal</td>\n",
       "      <td>yes</td>\n",
       "      <td>flu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  headache    sore temperature cough CLASS\n",
       "0   severe    mild        high   yes   flu\n",
       "1       no  severe      normal   yes  cold\n",
       "2     mild    mild      normal   yes   flu\n",
       "3     mild      no      normal    no  cold\n",
       "4   severe  severe      normal   yes   flu"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "\n",
    "def preprocess(dataframe, datatype): \n",
    "    # Create a copy of the original dataframe. Remove rows with ? in this copy\n",
    "    df = dataframe\n",
    "\n",
    "    # If it's mushroom data, drop the stalk-root column as it has too many missing values. Don't use this attribute\n",
    "    if(filename == \"datasets/mushroom.data\"):\n",
    "        df = df.drop(columns = ['stalk-root'])\n",
    "\n",
    "    # Otherwise just remove the entries with a ? for now\n",
    "    for index in df.index:\n",
    "        if('?' in df.loc[index].values):\n",
    "            df = df.drop(index)\n",
    "    \n",
    "    # Leave nominal, ordinal and numeric data as it is.\n",
    "    \n",
    "    # If the data is mixed, we need to discretise the data, as each mixed data set is a classification task using NB\n",
    "    if(datatype == 'mixed'):\n",
    "        df = discretiseNumeric(df)\n",
    "        \n",
    "    # Also, move CLASS column to the end\n",
    "    temp = df.pop('CLASS')\n",
    "    df['CLASS'] = temp\n",
    "    \n",
    "    return df\n",
    "\n",
    "############################################################################################################################\n",
    "# Choice is to use equal-frequency discretisation. More detailed than equal-width discretisation. Here, computing an \n",
    "# \"Optimal\" number of clusters for k means clustering for every numeric column would not be feasible. If choosing an \n",
    "# Arbitrary K clusters for every single discretisation, then it isn't as effective\n",
    "# For equal-frequency, sometimes the attributes won't have many values, so migth be less than 5 bins. Thats okay though\n",
    "\n",
    "def discretiseNumeric(df):\n",
    "    for column in df.columns:\n",
    "        if(df.dtypes[column] == \"int64\" or df.dtypes[column] == \"float64\"):\n",
    "            values = df[column].values\n",
    "            values.sort()\n",
    "            \n",
    "            # Check if the values are already \"Discretised\" (<, say, 8 unique values. This occurs for ratings 0-5 etc..).\n",
    "            # If so, then no need to discretise the attribute\n",
    "            uniqueValues = set(values)\n",
    "            if(len(uniqueValues) <= 8):\n",
    "                continue\n",
    "            \n",
    "            # Choose 5 equal-frequency bins, based on sorted values\n",
    "            # Should every attribute be split into 5 bins?\n",
    "            \n",
    "            bin1 = values[(int)(len(values)/5)]\n",
    "            bin2 = values[2 * (int)(len(values)/5)]\n",
    "            bin3 = values[3 * (int)(len(values)/5)]\n",
    "            bin4 = values[4 * (int)(len(values)/5)]\n",
    "            bin5 = values[5 * (int)(len(values)/5)]\n",
    "\n",
    "            # Now assign every value to be in a bin number\n",
    "            for i in range(len(df[column].values)):\n",
    "                if(df[column].values[i] <= bin1):\n",
    "                    df[column].values[i] = 1\n",
    "                elif(df[column].values[i] <= bin2):\n",
    "                    df[column].values[i] = 2\n",
    "                elif(df[column].values[i] <= bin3):\n",
    "                    df[column].values[i] = 3\n",
    "                elif(df[column].values[i] <= bin4):\n",
    "                    df[column].values[i] = 4\n",
    "                elif(df[column].values[i] <= bin5):\n",
    "                    df[column].values[i] = 5\n",
    "                \n",
    "    return df\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "df2 = dataframe.copy()\n",
    "df2 = preprocess(df2, datatype)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculate prior probabilities and likelihoods from the training data and using\n",
    "# them to build a naive Bayes model\n",
    "\n",
    "def train(dataframe, datatype):\n",
    "    \n",
    "    if(datatype in [\"nominal\", \"ordinal\", \"mixed\"]):\n",
    "        modeltype = 'gaussianNB'\n",
    "    elif(datatype in [\"numeric\"]):\n",
    "        modeltype = 'NB'\n",
    "    \n",
    "    #for column in dataframe.columns:\n",
    "    #    print(column)\n",
    "    #    print(len((set(dataframe[column].values))))\n",
    "        \n",
    "    #####################################################################################################################\n",
    "    # code for normal naive bayes\n",
    "    # Calculate all probabilities from training data\n",
    "    # Make a 1d list of dictionaries of dictionaries. Plan structure and order for calculating the probabilities\n",
    "    # Slice out rows in dataframe. .shape[0] gives num rows.\n",
    "    \n",
    "    trainNB(dataframe, datatype)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flu': {'no': 0.0, 'severe': 0.6666666666666666, 'mild': 0.3333333333333333}, 'cold': {'no': 0.5, 'severe': 0.0, 'mild': 0.5}}\n",
      "{'flu': {'no': 0.0, 'severe': 0.3333333333333333, 'mild': 0.6666666666666666}, 'cold': {'no': 0.5, 'severe': 0.5, 'mild': 0.0}}\n",
      "{'flu': {'high': 0.3333333333333333, 'normal': 0.6666666666666666}, 'cold': {'high': 0.0, 'normal': 1.0}}\n",
      "{'flu': {'no': 0.0, 'yes': 1.0}, 'cold': {'no': 0.5, 'yes': 0.5}}\n",
      "{'flu': {'flu': 1.0, 'cold': 0.0}, 'cold': {'flu': 0.0, 'cold': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "def trainNB(dataframe, datatype): \n",
    "    \n",
    "    # Count all the frequencies of each class label\n",
    "    priors = dict()\n",
    "    for index in dataframe.index:\n",
    "        item = dataframe.loc[index]['CLASS']\n",
    "        if(item in priors):\n",
    "            priors[item] += 1\n",
    "        else:\n",
    "            priors[item] = 1\n",
    "            \n",
    "    # Sum the total to convert each class label to a probability\n",
    "    total = sum(priors.values())\n",
    "    \n",
    "    # Each unique class label now has a prior probability as a value\n",
    "    for key, value in priors.items():\n",
    "        priors[key] = value/total\n",
    "        \n",
    "    # Once we have the priors, we need all the conditional probabilities\n",
    "    # Use a list of dictionaries of dictionaries\n",
    "    \n",
    "    # We need these to iterate through them and count instances\n",
    "    attributes = list(dataframe.columns)\n",
    "    uniqueClassLabels = list(set(dataframe['CLASS'].values))\n",
    "    \n",
    "    attributeDicts = list()\n",
    "    \n",
    "    # For each attribute create a dictionary with class labels as keys.\n",
    "    # Values are another dictionary with unique values in that attribute,\n",
    "    # Then count frequencies of uniqueattvalues given class label\n",
    "    for attribute in attributes:\n",
    "        dict1 = dict()\n",
    "        uniqueAttributeValues = list(set(dataframe[attribute].values))\n",
    "        for classLabel in uniqueClassLabels:\n",
    "            dict2 = dict()\n",
    "            dict1[classLabel] = dict2\n",
    "            for attributeValue in uniqueAttributeValues:\n",
    "                dict2[attributeValue] = dataframe[(dataframe[attribute] == attributeValue) & (dataframe['CLASS'] == classLabel)].shape[0]\n",
    "        attributeDicts.append(dict1)\n",
    "    \n",
    "    # Once we have the frequencies, can just convert each to be probabilities\n",
    "    for attributedict in attributeDicts:\n",
    "        for classLabel, classLabelDict in attributedict.items():\n",
    "            totalFreq = sum(classLabelDict.values())\n",
    "            for attributeLevel, freq in classLabelDict.items():\n",
    "                classLabelDict[attributeLevel] = freq / totalFreq\n",
    "\n",
    "    # EACH DICTIONARY IN attributeDicts IS IN ORDER OF THE HEADER COLUMNS\n",
    "    for dictionary in attributeDicts:\n",
    "        print(dictionary)\n",
    "    \n",
    "    \n",
    "    return attributeDicts\n",
    "\n",
    "train(df2, datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGNB(dataframe, datatype):\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict classes for new items in a test dataset (for the purposes of this assignment, you\n",
    "# can re-use the training data as a test set)\n",
    "\n",
    "def predict():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaliate the prediction performance by comparing your model’s class outputs to ground\n",
    "# truth labels\n",
    "\n",
    "def evaluate():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "\n",
    "\n",
    "If you are in a group of 1, you will respond to question (1), and **one** other of your choosing (two responses in total).\n",
    "\n",
    "If you are in a group of 2, you will respond to question (1) and question (2), and **two** others of your choosing (four responses in total). \n",
    "\n",
    "A response to a question should take about 100–250 words, and make reference to the data wherever possible.\n",
    "\n",
    "#### NOTE: you may develope codes or functions in respond to the question, but your formal answer should be added to a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Try discretising the numeric attributes in these datasets and treating them as discrete variables in the na¨ıve Bayes classifier. You can use a discretisation method of your choice and group the numeric values into any number of levels (but around 3 to 5 levels would probably be a good starting point). Does discretizing the variables improve classification performance, compared to the Gaussian na¨ıve Bayes approach? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Implement a baseline model (e.g., random or 0R) and compare the performance of the na¨ıve Bayes classifier to this baseline on multiple datasets. Discuss why the baseline performance varies across datasets, and to what extent the na¨ıve Bayes classifier improves on the baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Since it’s difficult to model the probabilities of ordinal data, ordinal attributes are often treated as either nominal variables or numeric variables. Compare these strategies on the ordinal datasets provided. Deterimine which approach gives higher classification accuracy and discuss why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy (you should implement this yourself and do not simply call existing implementations from `scikit-learn`). How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the na¨ıve Bayes classifier? Explain why, or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "The Gaussian na¨ıve Bayes classifier assumes that numeric attributes come from a Gaussian distribution. Is this assumption always true for the numeric attributes in these datasets? Identify some cases where the Gaussian assumption is violated and describe any evidence (or lack thereof) that this has some effect on the NB classifier’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
